<div align="center">
    <h1>Learning Where to Look: Self-supervised Viewpoint Selection for Active Localization using Geometrical Information</h1>
    <h3>ECCV 2024</h3>
    <div align="center">
        <a href="https://github.com/rvp-group/learning-where-to-look"><img src="assets/pipeline.png"/></a>   
    </div>
    Given a Structure-from-Motion model, we aim to learn the camera viewpoint that can be employed to maximize the accuracy in visual localization. 
    Our methodology requires first sampling the camera locations and orientation, calculating the best visibility orientation for each location,
    and learning active viewpoint through a Multi-layer Perceptron encoder. The illustration above shows our full pipeline predicting active viewpoints for visual 
    localization embedded into a planning framework.
    <br />   
    <br />   
    We will soon release the code; we need time to clean it and make it understandable! For now you can check our <a href="https://arxiv.org/abs/2407.15593">preprint</a>
  - accepted ECCV 2024.
</div>
